{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co to są *pipelines* w sci-kit learn i jak je wykorzystać?\n",
    "Czyli **bardziej efektywne szukanie najlepszego modelu**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli zajmujesz się tworzeniem modeli na przykład klasyfikujących jakieś dane to pewnie wielokrotnie powtarzasz te same kroki:\n",
    "\n",
    "* wczytanie danych\n",
    "* oczyszczenie danych\n",
    "* uzupełnienie braków\n",
    "* przygotowanie dodatkowych cech (*feature engineering*)\n",
    "* podział danych na treningowe i testowe\n",
    "* dobór hyperparametrów modelu\n",
    "* trenowanie modelu\n",
    "* testowanie modelu (sprawdzenie skuteczności działania)\n",
    "\n",
    "I tak w kółko, z każdym nowy modelem.\n",
    "\n",
    "Jest to dość nudne i dość powtarzalne. Szczególnie jak trzeba wykonać różne kroki transformacji danych w różnej kolejności - *upierdliwe* staje się zmienianie kolejności w kodzie.\n",
    "\n",
    "Dlatego wymyślono **pipelines**.\n",
    "\n",
    "Dlatego w tym i kolejnych postach zajmiemy się tym mechanizmem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczniemy od podstaw *data science*, czyli..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bez tego nie ma data science! ;)\n",
    "import pandas as pd\n",
    "\n",
    "# być może coś narysujemy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie elementy jakich będziemy używać znajdują się w ramach biblioteki **scikit-learn**. Zaimportujemy co trzeba plus kilka modeli z oddzielnych bibliotek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modele\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# preprocessing\n",
    "## zmienne ciągłe\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "## zmienne kategoryczne\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# dodatkowe modele spoza sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skąd wziąć dane? Można użyć wbudowanych w sklearna *irysów* czy *boston housing* ale mi zależało na znalezieniu takiego datasetu, który będzie zawierał cechy zarówno ciągłe jak i kategoryczne. Takim zestawem jest **Adult** znany też jako **Census Income**, a ściągnąć go można z [UCI](https://archive.ics.uci.edu/ml/datasets/Adult) (studenci i absolwenci AGH mogą mylić z [UCI](https://historia.agh.edu.pl/wiki/Uczelniane_Centrum_Informatyki) w budynku C-1).\n",
    "\n",
    "Pobieramy (potrzebne nam będą pliki [adult.data](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data) oraz [adult.test](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test)) dane, wrzucamy surowe pliki do katalogu *data/* i wczytujemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dane nie mają nagłówka - samo sobie nadamy nazwy kolumn\n",
    "col_names= ['age', 'work_class', 'final_weight', 'education', 'education_num',\n",
    "            'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "            'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "            'year_income']\n",
    "\n",
    "# wczytujemy dane\n",
    "adult_dataset = pd.read_csv(\"data/adult.data\",\n",
    "                            engine='python', sep=', ', # tu jest przeciek i spacja!\n",
    "                            header=None, names=col_names,\n",
    "                            na_values=\"?\")\n",
    "\n",
    "# kolumna 'final_weight' do niczego się nie przyda, więc od razu ją usuwamy\n",
    "# wiadomo to z EDA, które tutaj pomijamy\n",
    "adult_dataset.drop('final_weight', axis=1, inplace=True)\n",
    "\n",
    "# usuwamy braki, żeby uprościć przykład\n",
    "adult_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jakie mamy typy danych w kolumnach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "work_class        object\n",
       "education         object\n",
       "education_num      int64\n",
       "marital_status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital_gain       int64\n",
       "capital_loss       int64\n",
       "hours_per_week     int64\n",
       "native_country    object\n",
       "year_income       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz standardowo - dzielimy dane na zbiór treningowy i testowy. Przy okazji z całej ramki danych wyciągamy kolumnę `year_income` jako **Y**, a resztę jako **X**. Szalenie wygodnym jest nazywanie cech zmienną **X** a *targetów* **y** - przy *Ctrl-C + Ctrl-V* ze StackOverflow niczego właściwie nie trzeba robić ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(adult_dataset.drop('year_income', axis=1),\n",
    "                                                    adult_dataset['year_income'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sporą część wstępnej analizy pomijam w tym wpisie, ale jeśli nie wiesz dlaczego wybieram takie a nie inne kolumny to zrób samodzielnie analizę danych w tym zbiorze. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Czas na trochę informacji o *rurociągach*.**\n",
    "\n",
    "W dużym uproszszczeniu są to połączone sekwencyjnie (jedna za drugą, wyjście pierwszej trafia na wejście drugiej i tak dalej do końca) operacje. Operacje czyli klasy, które posiadają metody `.fit()` i `.transform()`.\n",
    "\n",
    "Rurociąg może składać się z kilku rurociągów połączonych jeden za drugim - taki model zastosujemy za chwilę.\n",
    "\n",
    "Powiedzieliśmy sobie wyżej, że pierwszy krok to przygotowanie danych - odpowiednie transformacje danych źródłowych i ewentualnie uzupełnienie danych brakujących. W dzisiejszym przkładzie brakujące dane (około 7% całości) po prostu wyrzuciliśmy przez `dropna()`, więc uzupełnieniem braków się nie zajmujemy. Ale może w kolejnej części już tak.\n",
    "\n",
    "Drugim krokiem jest przesłanie danych odpowiednio obrobionych do modelu i jego wytrenowanie.\n",
    "\n",
    "**No to do dzieła, już konkretnie - transformacja danych**\n",
    "\n",
    "Zatem na początek przygotujemy sobie fragmenty całego *rurociągu* odpowiedzialnego za transformacje kolumn. Mamy dwa typy kolumn, zatem zbudujemy dwa małe rurociągi.\n",
    "\n",
    "Pierwszy będzie odpowiedzialny za kolumny z wartościami liczbowymi. Nie wiemy czy są to wartości ciągłe (jak na przykład wiek) czy dyskretne (tutaj taką kolumną jest `education_num` mówiąca o poziome edukacji) i poniżej bierzemy wszystkie jak leci. Znowu: porządna EDA wskaże nam odpowiednie kolumny.\n",
    "\n",
    "Najpierw wybieramy wszystkie kolumny o typie numerycznym, a potem budujemy mini-rurociąg `transformer_numerical`, którego jedynym krokiem będzie wywołanie `StandardScaler()` zapisane pod nazwą `num_trans` (to musi być unikalne w całym procesie). Kolejny krok łatwo dodać - po prostu dodajemy kolejnego *tupla* w takim samym schemacie.\n",
    "\n",
    "**Co nam to daje?** Ano daje to tyle, że mamy konkretną nazwę dla konkretnego kroku. Później możemy się do niej dostać i na przykład zmienić: zarówno metodę wywoływaną w tym konkretnym kroku jak i parametry tej metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista kolumn numerycznych\n",
    "cols_numerical = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# transformer dla kolumn numerycznych\n",
    "transformer_numerical = Pipeline(steps = [\n",
    "    ('num_trans', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To samo robimy dla kolumn z wartościami kategorycznymi - budujemy mini-rurociąg `transformer_categorical`, który w kroku `cat_trans` wywołuje `OneHotEncoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista kolmn kategorycznych\n",
    "cols_categorical = ['work_class', 'education', 'marital_status', 'occupation',\n",
    "                    'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "# transformer dla kolumn numerycznych\n",
    "transformer_categorical = Pipeline(steps = [\n",
    "    ('cat_trans', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co dalej? Z tych dwóch małych rurociągów zbudujemy większy - `preprocessor`. Właściwie to będzie to swego rodzaju rozgałęzienie - **ColumnTransformer** który jedne kolumny puści jednym mini-rurociągiem, a drugie - drugim. I znowu: tutaj może być kilka elementów, oddzielne *przepływy* dla konkretnych kolumn (bo może jedne ciągłe chcemy skalować w jeden sposób, a inne w inny? A może jedne zmienne chcemy uzupełnić średnią a inne medianą?) - mamy pełną swobodę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesor danych\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('numerical', transformer_numerical, cols_numerical),\n",
    "    ('categorical', transformer_categorical, cols_categorical)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cała rura to złożenie odpowiednich elementów w całość - robiliśmy to już wyżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', RandomForestClassifier())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz **cały proces** wygląda następująco:\n",
    "\n",
    "* najpierw preprocessing:\n",
    "    * dla kolumn liczbowych wykonywany jest `StandardScaler()`\n",
    "    * dla kolumn kategorycznych - `OneHotEncoder()`\n",
    "* złożone dane przekazywane są do `RandomForestClassifier()`\n",
    "\n",
    " \n",
    " Proces trenuje się dokładne tak samo jak model - poprzez wywołanie metody `.fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('numerical',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('num_trans',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  with_mean=True,\n",
       "                                                                                  with_std=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  Index(['age', 'education_num', 'capital_gain', 'capital_loss',\n",
       "       'hours_per_wee...\n",
       "                                                   'marital_status',\n",
       "                                                   'occupation', 'relationship',\n",
       "                                                   'race', 'sex',\n",
       "                                                   'native_country'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=500,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście predykcja działa tak samo *jak zawsze*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '<=50K', '<=50K', ..., '>50K', '<=50K', '<=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Są też metody zwracające prawdopodobieństwo przypisania do każdej z klas `.predict_proba()` oraz jego logarytn `.predict_log_proba()`.\n",
    "\n",
    "Po wytrenowaniu na danych treningowych (cechy *X_train*, target *y_train*) możemy zobaczyć ocenę modelu na danych testowych (odpowiednio *X_test* i *y_test*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850259697204111"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Świetnie, świetne, ale to samo można bez tych pipelinów, nie raz na Kaggle tak robili i działało. Więc **po co to wszystko?**\n",
    "\n",
    "Ano po to, co nastąpi za chwilę.\n",
    "\n",
    "Mamy cały proces, każdy jego krok ma swoją nazwę, prawda? A może zamiast *StandardScaler()* lepszy będzie *MinMaxScaler()*? A może inna klasa modeli (zamiast lasów losowych np. XGBoost?). A gdyby sprawdzić każdy model z każdą transformacją? No to się robi sporo kodu... A nazwane kroki w procesie pozwalają na prostą podmiankę!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy sobie przestrzeń poszukiwań najlepszzego modelu i najlepszych transformacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasyfikatory                            \n",
    "classifiers = [\n",
    "    DummyClassifier(strategy='stratified'),\n",
    "    LogisticRegression(max_iter=500), # można tutaj podać hiperparametry\n",
    "    KNeighborsClassifier(2), # 2 bo mamy dwie klasy\n",
    "    ExtraTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    XGBClassifier(),\n",
    "    CatBoostClassifier(silent=True),\n",
    "    LGBMClassifier(verbose=-1)\n",
    "]\n",
    "\n",
    "# transformatory dla kolumn liczbowych\n",
    "scalers = [StandardScaler(), MinMaxScaler(), Normalizer()]\n",
    "\n",
    "# transformatory dla kolumn kategorycznych\n",
    "cat_transformers = [OrdinalEncoder(), OneHotEncoder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz w zagnieżdżonych pętlach możemy sprawdzić *każdy z każdym* podmieniając klasyfikatory i transformatory (cała pętla trochę się kręci):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 62.29%, Time: 0.1059 s\n",
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 62.52%, Time: 0.1041 s\n",
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 62.46%, Time: 0.1132 s\n",
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 62.8%, Time: 0.1334 s\n",
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 62.55%, Time: 0.08864 s\n",
      "========================================\n",
      "{'model': 'DummyClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 62.47%, Time: 0.1308 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 81.79%, Time: 2.772 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 85.03%, Time: 1.526 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 81.63%, Time: 9.622 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 84.86%, Time: 2.194 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 78.35%, Time: 8.648 s\n",
      "========================================\n",
      "{'model': 'LogisticRegression', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 84.21%, Time: 1.529 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 81.04%, Time: 0.8799 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 81.16%, Time: 0.1355 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 79.89%, Time: 0.6737 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 80.05%, Time: 0.1117 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 80.72%, Time: 0.6573 s\n",
      "========================================\n",
      "{'model': 'KNeighborsClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 80.99%, Time: 0.1097 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 79.89%, Time: 0.3407 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 79.3%, Time: 0.6547 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 80.0%, Time: 0.1258 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 79.24%, Time: 0.4838 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 79.47%, Time: 0.1251 s\n",
      "========================================\n",
      "{'model': 'ExtraTreeClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 80.25%, Time: 0.4519 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 84.77%, Time: 3.17 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 84.3%, Time: 23.65 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 84.74%, Time: 2.665 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 84.15%, Time: 21.63 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 84.02%, Time: 4.26 s\n",
      "========================================\n",
      "{'model': 'RandomForestClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 84.3%, Time: 19.52 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 79.78%, Time: 19.81 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 85.32%, Time: 36.17 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 74.78%, Time: 21.95 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 83.42%, Time: 35.68 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 76.94%, Time: 25.77 s\n",
      "========================================\n",
      "{'model': 'SVC', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 83.63%, Time: 36.49 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 87.03%, Time: 65.86 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 87.1%, Time: 59.82 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 87.03%, Time: 5.584 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 86.93%, Time: 11.55 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 85.7%, Time: 43.9 s\n",
      "========================================\n",
      "{'model': 'XGBClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 86.18%, Time: 27.29 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 87.15%, Time: 83.52 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 87.14%, Time: 54.58 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 87.15%, Time: 54.94 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 87.14%, Time: 50.58 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 85.69%, Time: 54.08 s\n",
      "========================================\n",
      "{'model': 'CatBoostClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 85.93%, Time: 66.99 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 86.89%, Time: 3.265 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'StandardScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 87.09%, Time: 1.521 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 86.89%, Time: 0.9862 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'MinMaxScaler', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 87.09%, Time: 21.55 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OrdinalEncoder'}\n",
      "Score: 85.87%, Time: 2.244 s\n",
      "========================================\n",
      "{'model': 'LGBMClassifier', 'num_trans': 'Normalizer', 'cat_trans': 'OneHotEncoder'}\n",
      "Score: 85.91%, Time: 1.135 s\n"
     ]
    }
   ],
   "source": [
    "# miejsce na zebranie wyników\n",
    "models_df = pd.DataFrame()\n",
    "\n",
    "# przygotowujemy pipeline\n",
    "pipe = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor), # mniejszy pipeline\n",
    "    ('classifier', None) # to ustalimy za moment\n",
    "])\n",
    "\n",
    "# dla każdego typu modelu zmieniamy kolejne transformatory kolumn\n",
    "for model in classifiers:\n",
    "    for num_tr in scalers:\n",
    "        for cat_tr in cat_transformers:\n",
    "            # odpowiednio zmieniamy jego paramety - dobieramy transformatory\n",
    "            pipe_params = {\n",
    "                'preprocessor__numerical__num_trans': num_tr,\n",
    "                'preprocessor__categorical__cat_trans': cat_tr,\n",
    "                'classifier': model\n",
    "            }\n",
    "            pipe.set_params(**pipe_params)\n",
    "\n",
    "            # zbieramy w dict parametry dla Pipeline\n",
    "            param_dict = {\n",
    "                        'model': model.__class__.__name__,\n",
    "                        'num_trans': num_tr.__class__.__name__,\n",
    "                        'cat_trans': cat_tr.__class__.__name__\n",
    "                    }\n",
    "\n",
    "            # wypisujemy je\n",
    "            print('='*40)\n",
    "            print(param_dict)\n",
    "            \n",
    "            # trenujemy tak przygotowany model (cały pipeline) mierząc ile to trwa\n",
    "            start_time = time.time()\n",
    "            pipe.fit(X_train, y_train)   \n",
    "            end_time = time.time()\n",
    "\n",
    "            # sprawdzamy jak wyszło\n",
    "            score = pipe.score(X_test, y_test)\n",
    "\n",
    "            # i informyjemy o tym\n",
    "            print(f\"Score: {round(100*score, 2)}%, Time: {(end_time-start_time):.3} s\")\n",
    "\n",
    "            # i dodajemy do tabelki wszystkich wyników (razem z parametrami)\n",
    "            param_dict['score'] = score\n",
    "            param_dict['time_elapsed'] = end_time - start_time\n",
    "            \n",
    "            models_df = models_df.append(pd.DataFrame(param_dict, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz w jednej tabeli mamy wszystkie interesujące dane, które mogą posłużyć nam chociażby do znalezienia najlepszego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_trans</th>\n",
       "      <th>cat_trans</th>\n",
       "      <th>score</th>\n",
       "      <th>time_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.871478</td>\n",
       "      <td>54.939374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.871478</td>\n",
       "      <td>83.523171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.871367</td>\n",
       "      <td>54.575908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.871367</td>\n",
       "      <td>50.578351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.871035</td>\n",
       "      <td>59.816833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.870925</td>\n",
       "      <td>21.554955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.870925</td>\n",
       "      <td>1.521320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.870262</td>\n",
       "      <td>65.858596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.870262</td>\n",
       "      <td>5.584308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.869267</td>\n",
       "      <td>11.546939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>3.265042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.861753</td>\n",
       "      <td>27.290781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.859321</td>\n",
       "      <td>66.990122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>1.134837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.858658</td>\n",
       "      <td>2.244430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>43.900374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.856890</td>\n",
       "      <td>54.077270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.853243</td>\n",
       "      <td>36.167002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.850260</td>\n",
       "      <td>1.525752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.848602</td>\n",
       "      <td>2.193768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.847718</td>\n",
       "      <td>3.170452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.847386</td>\n",
       "      <td>2.665419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.842966</td>\n",
       "      <td>19.521414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.842966</td>\n",
       "      <td>23.646469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.842082</td>\n",
       "      <td>1.528877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.841529</td>\n",
       "      <td>21.628550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.840203</td>\n",
       "      <td>4.259819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.836336</td>\n",
       "      <td>36.487335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.834236</td>\n",
       "      <td>35.676270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.817880</td>\n",
       "      <td>2.772133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.816333</td>\n",
       "      <td>9.621832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.811581</td>\n",
       "      <td>0.135454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.810366</td>\n",
       "      <td>0.879905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.809924</td>\n",
       "      <td>0.109682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.807161</td>\n",
       "      <td>0.657308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.802520</td>\n",
       "      <td>0.451857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.800530</td>\n",
       "      <td>0.111691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.799978</td>\n",
       "      <td>0.125797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.798873</td>\n",
       "      <td>0.673653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.798873</td>\n",
       "      <td>0.340745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.797768</td>\n",
       "      <td>19.805745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.794673</td>\n",
       "      <td>0.125123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.793016</td>\n",
       "      <td>0.654658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.792353</td>\n",
       "      <td>0.483842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.783512</td>\n",
       "      <td>8.647921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.769367</td>\n",
       "      <td>25.773245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.747817</td>\n",
       "      <td>21.946773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.628025</td>\n",
       "      <td>0.133445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.625483</td>\n",
       "      <td>0.088644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.625152</td>\n",
       "      <td>0.104141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Normalizer</td>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.624710</td>\n",
       "      <td>0.130823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.624599</td>\n",
       "      <td>0.113186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.622942</td>\n",
       "      <td>0.105865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model       num_trans       cat_trans     score  \\\n",
       "0      CatBoostClassifier    MinMaxScaler  OrdinalEncoder  0.871478   \n",
       "0      CatBoostClassifier  StandardScaler  OrdinalEncoder  0.871478   \n",
       "0      CatBoostClassifier  StandardScaler   OneHotEncoder  0.871367   \n",
       "0      CatBoostClassifier    MinMaxScaler   OneHotEncoder  0.871367   \n",
       "0           XGBClassifier  StandardScaler   OneHotEncoder  0.871035   \n",
       "0          LGBMClassifier    MinMaxScaler   OneHotEncoder  0.870925   \n",
       "0          LGBMClassifier  StandardScaler   OneHotEncoder  0.870925   \n",
       "0           XGBClassifier  StandardScaler  OrdinalEncoder  0.870262   \n",
       "0           XGBClassifier    MinMaxScaler  OrdinalEncoder  0.870262   \n",
       "0           XGBClassifier    MinMaxScaler   OneHotEncoder  0.869267   \n",
       "0          LGBMClassifier    MinMaxScaler  OrdinalEncoder  0.868936   \n",
       "0          LGBMClassifier  StandardScaler  OrdinalEncoder  0.868936   \n",
       "0           XGBClassifier      Normalizer   OneHotEncoder  0.861753   \n",
       "0      CatBoostClassifier      Normalizer   OneHotEncoder  0.859321   \n",
       "0          LGBMClassifier      Normalizer   OneHotEncoder  0.859100   \n",
       "0          LGBMClassifier      Normalizer  OrdinalEncoder  0.858658   \n",
       "0           XGBClassifier      Normalizer  OrdinalEncoder  0.857001   \n",
       "0      CatBoostClassifier      Normalizer  OrdinalEncoder  0.856890   \n",
       "0                     SVC  StandardScaler   OneHotEncoder  0.853243   \n",
       "0      LogisticRegression  StandardScaler   OneHotEncoder  0.850260   \n",
       "0      LogisticRegression    MinMaxScaler   OneHotEncoder  0.848602   \n",
       "0  RandomForestClassifier  StandardScaler  OrdinalEncoder  0.847718   \n",
       "0  RandomForestClassifier    MinMaxScaler  OrdinalEncoder  0.847386   \n",
       "0  RandomForestClassifier      Normalizer   OneHotEncoder  0.842966   \n",
       "0  RandomForestClassifier  StandardScaler   OneHotEncoder  0.842966   \n",
       "0      LogisticRegression      Normalizer   OneHotEncoder  0.842082   \n",
       "0  RandomForestClassifier    MinMaxScaler   OneHotEncoder  0.841529   \n",
       "0  RandomForestClassifier      Normalizer  OrdinalEncoder  0.840203   \n",
       "0                     SVC      Normalizer   OneHotEncoder  0.836336   \n",
       "0                     SVC    MinMaxScaler   OneHotEncoder  0.834236   \n",
       "0      LogisticRegression  StandardScaler  OrdinalEncoder  0.817880   \n",
       "0      LogisticRegression    MinMaxScaler  OrdinalEncoder  0.816333   \n",
       "0    KNeighborsClassifier  StandardScaler   OneHotEncoder  0.811581   \n",
       "0    KNeighborsClassifier  StandardScaler  OrdinalEncoder  0.810366   \n",
       "0    KNeighborsClassifier      Normalizer   OneHotEncoder  0.809924   \n",
       "0    KNeighborsClassifier      Normalizer  OrdinalEncoder  0.807161   \n",
       "0     ExtraTreeClassifier      Normalizer   OneHotEncoder  0.802520   \n",
       "0    KNeighborsClassifier    MinMaxScaler   OneHotEncoder  0.800530   \n",
       "0     ExtraTreeClassifier    MinMaxScaler  OrdinalEncoder  0.799978   \n",
       "0    KNeighborsClassifier    MinMaxScaler  OrdinalEncoder  0.798873   \n",
       "0     ExtraTreeClassifier  StandardScaler  OrdinalEncoder  0.798873   \n",
       "0                     SVC  StandardScaler  OrdinalEncoder  0.797768   \n",
       "0     ExtraTreeClassifier      Normalizer  OrdinalEncoder  0.794673   \n",
       "0     ExtraTreeClassifier  StandardScaler   OneHotEncoder  0.793016   \n",
       "0     ExtraTreeClassifier    MinMaxScaler   OneHotEncoder  0.792353   \n",
       "0      LogisticRegression      Normalizer  OrdinalEncoder  0.783512   \n",
       "0                     SVC      Normalizer  OrdinalEncoder  0.769367   \n",
       "0                     SVC    MinMaxScaler  OrdinalEncoder  0.747817   \n",
       "0         DummyClassifier    MinMaxScaler   OneHotEncoder  0.628025   \n",
       "0         DummyClassifier      Normalizer  OrdinalEncoder  0.625483   \n",
       "0         DummyClassifier  StandardScaler   OneHotEncoder  0.625152   \n",
       "0         DummyClassifier      Normalizer   OneHotEncoder  0.624710   \n",
       "0         DummyClassifier    MinMaxScaler  OrdinalEncoder  0.624599   \n",
       "0         DummyClassifier  StandardScaler  OrdinalEncoder  0.622942   \n",
       "\n",
       "   time_elapsed  \n",
       "0     54.939374  \n",
       "0     83.523171  \n",
       "0     54.575908  \n",
       "0     50.578351  \n",
       "0     59.816833  \n",
       "0     21.554955  \n",
       "0      1.521320  \n",
       "0     65.858596  \n",
       "0      5.584308  \n",
       "0     11.546939  \n",
       "0      0.986200  \n",
       "0      3.265042  \n",
       "0     27.290781  \n",
       "0     66.990122  \n",
       "0      1.134837  \n",
       "0      2.244430  \n",
       "0     43.900374  \n",
       "0     54.077270  \n",
       "0     36.167002  \n",
       "0      1.525752  \n",
       "0      2.193768  \n",
       "0      3.170452  \n",
       "0      2.665419  \n",
       "0     19.521414  \n",
       "0     23.646469  \n",
       "0      1.528877  \n",
       "0     21.628550  \n",
       "0      4.259819  \n",
       "0     36.487335  \n",
       "0     35.676270  \n",
       "0      2.772133  \n",
       "0      9.621832  \n",
       "0      0.135454  \n",
       "0      0.879905  \n",
       "0      0.109682  \n",
       "0      0.657308  \n",
       "0      0.451857  \n",
       "0      0.111691  \n",
       "0      0.125797  \n",
       "0      0.673653  \n",
       "0      0.340745  \n",
       "0     19.805745  \n",
       "0      0.125123  \n",
       "0      0.654658  \n",
       "0      0.483842  \n",
       "0      8.647921  \n",
       "0     25.773245  \n",
       "0     21.946773  \n",
       "0      0.133445  \n",
       "0      0.088644  \n",
       "0      0.104141  \n",
       "0      0.130823  \n",
       "0      0.113186  \n",
       "0      0.105865  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ale *najlepszy* może być w różnych kategoriach - nie tylko skuteczności, ale też na przykład czasu uczenia czy też stabilności wyniku. Zobaczmy podstawowe statystyki dla typów modeli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df[['model', 'score', 'time_elapsed']] \\\n",
    "    .groupby('model') \\\n",
    "    .aggregate({\n",
    "        'score': ['mean','std', 'min', 'max'],\n",
    "        'time_elapsed': ['mean','std', 'min', 'max']\n",
    "        }) \\\n",
    "    .reset_index() \\\n",
    "    .sort_values(('score', 'mean'), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj tak na prawdę nie mierzymy stabilności modelu - podajemy różnie przetworzone dane do tego samego modelu. Stablilność można zmierzyć puszczając na model fragmentaryczne dane, co można zautomatyzować porpzez *KFold*/*RepeatedKFold* (z sklearn.model_selection), ale dzisiaj nie o tym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy który rodzaj modelu daje najlepszą skuteczność:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=models_df, x='score', y='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnie trzy (XGBoost, LigthGBM i CatBoost) dają najlepsze wyniki i pewnie warto je brać pod uwagę w przyszłości.\n",
    "\n",
    "A czy są różnice pomiędzy transformatorami?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=models_df, x='score', y='num_trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=models_df, x='score', y='cat_trans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy tych danych wygląda, że właściwie nie ma większej różnicy (nie bijemy się tutaj o 0.01 punktu procentowego poprawy *accuracy* modelu). Może więc czas treningu jest istotny?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=models_df, x='time_elapsed', y='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy kilku liderów, ale z tych które dawały najlepsze wyniki warto wziąć pod uwagę XGBoosta i LightGMB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki przećwiczeniu kilku modeli mamy dwóch najbardziej efektywnych (czasowo) i efektownych (z najlepszym *accuracy*) kandydatów do dalszych prac. Wyszukanie ich to kilka linii kodu. Jeśli przyjdzie nam do głowy nowy model - dodajemy go do listy `classifiers`. Jeśli znajdziemy inny transformator - dopisujemy do listy `scalers` lub `cat_transformers`. Nie trzeba kopiować dużych kawałków kodu, nie trzeba właściwie pisać nowego kodu.\n",
    "\n",
    "Dokładnie tym samym sposobem możemy poszukać *hyperparametrów* dla konkretnego modelu i zestawu transformacji w **pipeline**. Ale to już w następnym odcinku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odpowiednio zmieniamy jego paramety - dobieramy transformatory\n",
    "pipe_params = {\n",
    "    'preprocessor__numerical__num_trans': StandardScaler(),\n",
    "    'preprocessor__categorical__cat_trans': OneHotEncoder(),\n",
    "    'classifier': LGBMClassifier()\n",
    "}\n",
    "\n",
    "pipe.set_params(**pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, RepeatedStratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipe, X_train, y_train, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {scores.mean()}, median: {np.median(scores)}, std: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*scores.std()/scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=list(range(0, len(scores))), y=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
