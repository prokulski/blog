{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting the number of orders based on the international competition Global Manafement Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abstract\n",
    "2. About dataset\n",
    "3. Import librariers\n",
    "4. Prepare data\n",
    "5. Exploratory data\n",
    "6. Train Test Split\n",
    "7. Functions\n",
    "8. Custom Transformers\n",
    "9. Pipeline\n",
    "10. Models\n",
    "11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simulation of production company management. It is a simulation of the management of a manufacturing company, in which student and company teams participate. The teams play the role of the company management. The company is listed on the stock exchange. The role of the board is to make decisions based on quarterly reports. The companies operate internationally in three markets:\n",
    "* European Union\n",
    "* NAFTA - the United States, Canada and Mexico\n",
    "* Internet\n",
    "\n",
    "Teams are divided into up to eight teams. They compete to achieve the highest possible investment result at the end of the simulation. \n",
    "Each board makes **75 decisions**, including: production, research and development, sales, recruitment, marketing, investments, credits, dividend payment, issue or purchase of shares. The company produces and sells 3 different products in 3 different markets. \n",
    "\n",
    "<img src=\"graphics/01.jpg\" width=\"550\">\n",
    "\n",
    "Each team receives a report:\n",
    "\n",
    "* Resources and products\n",
    "\n",
    "<img src=\"graphics/02.jpg\" width=\"550\">\n",
    " \n",
    "* The financial statements and\n",
    "\n",
    "<img src=\"graphics/03.jpg\" width=\"550\">\n",
    " \n",
    "* Information about all companies in the group, the market situation and also information requested by the team.\n",
    "\n",
    "<img src=\"graphics/04.jpg\" width=\"550\">\n",
    "\n",
    "<img src=\"graphics/05.jpg\" width=\"550\">\n",
    "\n",
    "<img src=\"graphics/06.jpg\" width=\"550\">\n",
    "\n",
    "<img src=\"graphics/07.jpg\" width=\"550\">\n",
    "\n",
    "The most important factor is to maximize profit. This is done by selling products. By delivering more products than the customers actually order, we create stocks that we have to store, thus increasing losses, because the products produced by our company do not create added value. Another situation is to produce too few products. Therefore, **the most important thing is to forecast the number of products ordered**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. About dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set was created from 257 reports. The reports come from previous editions of my team and from the infected ones. Specific features were selected to build models. They were chosen on the basis of their field knowledge.\n",
    "\n",
    "The reports inside the dataset are:\n",
    "    - datasets/\n",
    "        - 01_reports/\n",
    "            - reports in xlsx files\n",
    "        - 02_all_reports_data/\n",
    "        - 03_all_reports_data_current_previous/\n",
    "        - 04_demand_datasets/\n",
    "    - graphics/\n",
    "    - models/\n",
    "    - scripts/\n",
    "\n",
    "Each file has a structured name:\n",
    "\n",
    "<img src=\"graphics/08.jpg\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of the necessery modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "# pipeline construction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# data processing\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# models trainng\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# optimization of hyperparameters\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, partial, space_eval\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# model evaluation\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# saving models\n",
    "import pickle\n",
    "\n",
    "# settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pio.renderers.default='notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 All  Reports Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all reports values were taken and created a file **all_reports_data.csv**\n",
    "\n",
    "<img src=\"graphics/09.jpg\" width=\"550\">\n",
    "\n",
    "The file was saved here:\n",
    "    - datasets/\n",
    "        - 01_reports/\n",
    "        - 02_all_reports_data/\n",
    "            - all_reports_data.csv\n",
    "        - 03_all_reports_data_current_previous/\n",
    "        - 04_demand_datasets/\n",
    "    - graphics/\n",
    "    - models/\n",
    "    - scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run -i 'scripts/01_prepare_all_reports_data/prepare_all_reports_data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 All Reports Data -> current and previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding values from the previous cycle.\n",
    "\n",
    "The file was saved here:\n",
    "    - datasets/\n",
    "        - 01_reports/\n",
    "        - 02_all_reports_data/\n",
    "        - 03_all_reports_data_current_previous/\n",
    "            - all_reports_data_current_previous.csv\n",
    "        - 04_demand_datasets/\n",
    "    - graphics/\n",
    "    - models/\n",
    "    - scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'scripts/02_prepare_all_reports_current_previous/prepare_all_reports_current_previous.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Demand Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, variables that affect the number of orders are taken from **all_reports_data_current_previous.csv**. The data is divided into 9 files depending on the product and area, because for them the variables are a little different. Information which variables to choose and how to prepare them was taken from the [blog](https://gmcworld.org/blog/demand-factors) and based on my own knowledge and experience. The functions were not taken because it was assumed that the model would detect them.\n",
    "\n",
    "<img src=\"graphics/10.jpg\" width=\"850\">\n",
    "\n",
    "The files was saved here:\n",
    "    - datasets/\n",
    "        - 01_reports/\n",
    "        - 02_all_reports_data/\n",
    "        - 03_all_reports_data_current_previous/\n",
    "        - 04_demand_datasets/\n",
    "            - Prod1_Europe.csv\n",
    "            - Prod1_Internet.csv\n",
    "            - Prod1_Nafta.csv\n",
    "            - Prod2_Europe.csv\n",
    "            - Prod2_Internet.csv\n",
    "            - Prod2_Nafta.csv\n",
    "            - Prod3_Europe.csv\n",
    "            - Prod3_Internet.csv\n",
    "            - Prod3_Nafta.csv\n",
    "    - graphics/\n",
    "    - models/\n",
    "    - scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'scripts/03_prepare_demand_datasets/prepare_demand_datasets.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Data connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine 9 files into one common file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/04_demand_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 1 Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod1_Europe = pd.read_csv(f'{path}Prod1_Europe.csv')\n",
    "Prod1_Europe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 1 Nafta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod1_Nafta = pd.read_csv(f'{path}Prod1_Nafta.csv')\n",
    "Prod1_Nafta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 1 Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod1_Internet = pd.read_csv(f'{path}Prod1_Internet.csv')\n",
    "Prod1_Internet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 2 Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod2_Europe = pd.read_csv(f'{path}Prod2_Europe.csv')\n",
    "Prod2_Europe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 2 Nafta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod2_Nafta = pd.read_csv(f'{path}Prod2_Nafta.csv')\n",
    "Prod2_Nafta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 2 Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod2_Internet = pd.read_csv(f'{path}Prod2_Internet.csv')\n",
    "Prod2_Internet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 3 Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod3_Europe = pd.read_csv(f'{path}Prod3_Europe.csv')\n",
    "Prod3_Europe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 3 Nafta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod3_Nafta = pd.read_csv(f'{path}Prod3_Nafta.csv')\n",
    "Prod3_Nafta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product 3 Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod3_Internet = pd.read_csv(f'{path}Prod3_Internet.csv')\n",
    "Prod3_Internet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_files = [\n",
    "    Prod1_Europe, Prod2_Europe, Prod3_Europe,\n",
    "    Prod1_Nafta, Prod2_Nafta, Prod3_Nafta,\n",
    "    Prod1_Internet, Prod2_Internet, Prod3_Internet\n",
    "]\n",
    "\n",
    "# Areas\n",
    "\n",
    "# Europe\n",
    "for file in list_of_files[:3]:\n",
    "    file['Support'] = 0\n",
    "    file['FailedVisits'] = 0\n",
    "    file['WebDev'] = 0\n",
    "    file['Area'] = 'Europe'\n",
    "\n",
    "# Nafta\n",
    "for file in list_of_files[3:6]:\n",
    "    file['Support'] = 0\n",
    "    file['FailedVisits'] = 0\n",
    "    file['WebDev'] = 0\n",
    "    file['Area'] = 'Nafta'\n",
    "\n",
    "# Internet\n",
    "for file in list_of_files[6:]:\n",
    "    file['Commission'] = 0\n",
    "    file['AgentsDistr'] = 0\n",
    "    file['Area'] = 'Internet'\n",
    "\n",
    "# Product 1\n",
    "for file in list_of_files[0::3]:\n",
    "    file['Product'] = 1\n",
    "\n",
    "# Product 2\n",
    "for file in list_of_files[1::3]:\n",
    "    file['Product'] = 2\n",
    "    \n",
    "# Product 3\n",
    "for file in list_of_files[2::3]:\n",
    "    file['Product'] = 3\n",
    "    \n",
    "data = pd.concat(list_of_files)\n",
    "data.drop(['Team', 'MarketShares_c'], axis=1, inplace=True)\n",
    "data = data[data['Cycle'] >= 5].reset_index(drop=True)\n",
    "data = data[data['NumOrders'] >= 10].reset_index(drop=True)\n",
    "data['History'] = pd.Categorical(data['History'])\n",
    "data['Cycle'] = pd.Categorical(data['Cycle'])\n",
    "data['Area'] = pd.Categorical(data['Area'])\n",
    "data['Product'] = pd.Categorical(data['Product'])\n",
    "\n",
    "# setting the first column of the explanatory variable\n",
    "col_name=\"NumOrders\"\n",
    "first_col = data.pop(col_name)\n",
    "data.insert(0, col_name, first_col)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by product and area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"Product\", \"Area\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **BOXPLOT** shows the distribution of the data with more detailed information. It shows the outliers more clearly, maximum, minimum, quartile(Q1), third quartile(Q3), interquartile range(IQR), and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    plt.figure(figsize=(14, 3))\n",
    "    data_selected = data.copy()\n",
    "    hue_selected = \"Area\"\n",
    "    if column == 'Commission':\n",
    "        data_selected = data_selected[data_selected['Area'] != 'Internet']\n",
    "    elif column == 'AgentsDistr':\n",
    "        data_selected = data_selected[data_selected['Area'] != 'Internet']\n",
    "    elif column == 'Training':\n",
    "        hue_selected = None\n",
    "    elif column == 'ManagBudget':\n",
    "        hue_selected = None\n",
    "    elif column == 'RandD':\n",
    "        hue_selected = None\n",
    "    elif column == 'AssemblyTime':\n",
    "        hue_selected = None\n",
    "    elif column == 'Support':\n",
    "        data_selected = data_selected[data_selected['Area'] == 'Internet']\n",
    "    elif column == 'FailedVisits':\n",
    "        data_selected = data_selected[data_selected['Area'] == 'Internet']\n",
    "    elif column == 'WebDev':\n",
    "        data_selected = data_selected[data_selected['Area'] == 'Internet']\n",
    "    sns.boxplot(x = \"Product\", y = column, hue = hue_selected, data = data_selected, palette = \"Blues\")\n",
    "    plt.xlabel('Product')\n",
    "    plt.ylabel(f'{column}')\n",
    "    if column not in ['Training', 'ManagBudget', 'RandD', 'AssemblyTime', 'Support', 'FailedVisits', 'WebDev']:\n",
    "        plt.legend(loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data have outliers, but they are real. This may indicate too few reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.select_dtypes(include=['category']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 4))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for (column, i) in zip(data.select_dtypes(include=['category']).columns, range(1, 5)):\n",
    "    ax = fig.add_subplot(1, 4, i)\n",
    "    data[column].value_counts().plot.bar()\n",
    "    plt.title(f'Column: {column}\\nUnique values: {len(data[column].unique())}')\n",
    "    plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more data for history number 3 and there is less data with each subsequent cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, nrows=4, figsize=(16, 16))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.select_dtypes(include=['int64', 'float64']).items():\n",
    "    sns.distplot(v, ax=axs[index]).set_title(f'\\nColumn: {k}\\nUnique values: {len(data[k].unique())}\\n')\n",
    "    index = index + 1\n",
    "for empty in range(17, 20):\n",
    "    axs[empty].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not normal distribution. You can see the division into products. Some data are skewed.\n",
    "\n",
    "But the Models allowed my team to reach the national final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['int64', 'float64']).columns[1:]:\n",
    "    print(f'NumOrders ({column})')\n",
    "    g = sns.FacetGrid(data, col=\"Product\", hue=\"Area\", height = 5)\n",
    "    g.map(plt.scatter, column, \"NumOrders\", alpha=.8)\n",
    "    g.add_legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(16, 11))\n",
    "ax = sns.heatmap(data.corr(), \n",
    "                 xticklabels=data.corr().columns, \n",
    "                 yticklabels=data.corr().columns, \n",
    "                 cmap='RdYlGn', \n",
    "                 center=0, \n",
    "                 annot=True)\n",
    "\n",
    "# Decorations\n",
    "plt.title('Correlogram of variables', fontsize=22)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scale(data.select_dtypes(include=['int64', 'float64']).drop(['NumOrders'], axis=1))\n",
    "pca_explained_data = PCA()\n",
    "pca_explained_data.fit(scaled_data)\n",
    "print(np.cumsum(pca_explained_data.explained_variance_ratio_))\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.bar(range(pca_explained_data.n_components_),np.cumsum(pca_explained_data.explained_variance_ratio_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"NumSales_p\", y=\"MarketShares_p\", \n",
    "           data=data[(data['Product']==1)&(data['MarketShares_p']!=0)], \n",
    "           fit_reg=True, hue='Area', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find dependence on such variables as **advertising**, **previous orders** or **price**, but the greatest influence has information about what **product** it is and on which **area** it is sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('NumOrders', axis = 1),\n",
    "                                                    data['NumOrders'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = data['Area'],\n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to evaluate models and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data_frame = pd.DataFrame(columns = [\n",
    "    'RMSE_train', 'Max_error_train', 'MAPE_train', 'RMSE_test', 'Max_error_test', 'MAPE_test'\n",
    "])\n",
    "\n",
    "def model_evaluation(model, name,\n",
    "                     X_test = X_test, X_train = X_train, y_test = y_test, y_train = y_train):\n",
    "    \n",
    "    global metrics_data_frame\n",
    "    \n",
    "    def hist_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - y\n",
    "        plt.hist(errors, bins = 100)\n",
    "        plt.title(f'Histogram of residuals - {set_name} set')\n",
    "    \n",
    "    def plot_of_residuals(X, y, set_name):\n",
    "        errors = model.predict(X) - y\n",
    "        plt.scatter(x = y, y = errors)\n",
    "        plt.axhline(0, color=\"r\", linestyle=\"--\")\n",
    "        plt.xlabel('True Value')\n",
    "        plt.ylabel('Residual')\n",
    "        plt.title(f'Plot of residuals - {set_name} set')\n",
    "    \n",
    "    def fit_scatter_plot(X, y, set_name):\n",
    "        y_fitted_values = model.predict(X)\n",
    "\n",
    "        xmin = y.min()\n",
    "        xmax = y.max()\n",
    "        plt.scatter(x = y_fitted_values, y = y)\n",
    "        x_line = np.linspace(xmin, xmax, 10)\n",
    "        y_line = x_line\n",
    "        plt.plot(x_line, y_line, 'r--')\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('True Value')\n",
    "        plt.title(f'Plot of predicted values versus true values - {set_name} set')\n",
    "        \n",
    "    def calculate_metrics(model = model, name = name,\n",
    "                          X_test = X_test, X_train = X_train, y_test = y_test, y_train = y_train):\n",
    "        \n",
    "        def mean_absolute_percentage_error(y_true, y_pred): \n",
    "            y_true, y_pred = y_true.ravel(), y_pred.ravel()\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        # quality of fit\n",
    "        fitted_values = model.predict(X_train)\n",
    "        rmse_train = np.sqrt(metrics.mean_squared_error(y_train, fitted_values))\n",
    "        max_error_train = np.max(np.abs(fitted_values - y_train))\n",
    "        mape_train = mean_absolute_percentage_error(y_true = y_train,\n",
    "                                                    y_pred = fitted_values)\n",
    "\n",
    "        # quality of prediction\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        max_error_test = np.max(np.abs(y_pred - y_test))\n",
    "        mape_test = mean_absolute_percentage_error(y_true = y_test,\n",
    "                                                   y_pred = y_pred)\n",
    "        \n",
    "        calculated_metrics = pd.DataFrame({\n",
    "            'RMSE_train': rmse_train, 'Max_error_train':max_error_train,'MAPE_train': mape_train,\n",
    "            'RMSE_test': rmse_test, 'Max_error_test':max_error_test, 'MAPE_test': mape_test\n",
    "        }, index = [name])\n",
    "                \n",
    "        return calculated_metrics\n",
    "    \n",
    "    fig = plt.figure(figsize = (16, 12))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 1)\n",
    "    fit_scatter_plot(X = X_train, y = y_train, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 2)\n",
    "    fit_scatter_plot(X = X_test, y = y_test, set_name = 'test')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 3)\n",
    "    plot_of_residuals(X = X_train, y = y_train, set_name = 'train')\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, 4)\n",
    "    plot_of_residuals(X = X_test, y = y_test, set_name = 'test')    \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 5)\n",
    "    hist_of_residuals(X = X_train, y = y_train, set_name = 'train')   \n",
    "\n",
    "    ax = fig.add_subplot(3, 2, 6)\n",
    "    hist_of_residuals(X = X_test, y = y_test, set_name = 'test')\n",
    "    \n",
    "    metrics_data_frame = metrics_data_frame.append(calculate_metrics())\n",
    "    return metrics_data_frame\n",
    "\n",
    "def save_model(model, name):\n",
    "    filename = f'models/{name}.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f'The model {name} has been saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_first = True, columns_to_left = None):\n",
    "        self.drop_first = drop_first\n",
    "        self.columns_to_left = columns_to_left\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X = pd.DataFrame(X)\n",
    "        if self.columns_to_left != None:\n",
    "            X_left = X[self.columns_to_left].astype(int).copy()\n",
    "        X = pd.get_dummies(X, drop_first = self.drop_first).copy()\n",
    "        if self.columns_to_left != None: \n",
    "            X = pd.concat([X, X_left], axis = 1)\n",
    "        return X\n",
    "\n",
    "# pokazac na wykresie wyzej zaleznosci dla Missing data\n",
    "class ImputeMissingMarketShares_p(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = [\n",
    "            'MarketShares_p', 'NumSales_p', \n",
    "            'History_1', 'History_2', 'History_3',\n",
    "            'Cycle_6', 'Cycle_7', 'Cycle_8', 'Cycle_9',\n",
    "            'Area_Internet', 'Area_Nafta',\n",
    "            'Product_2', 'Product_3'\n",
    "        ]\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        X_fit = X.copy()\n",
    "        filer_5_cycle = (X_fit['Cycle_6']==0)&(X_fit['Cycle_7']==0)&(X_fit['Cycle_8']==0)&(X_fit['Cycle_9']==0)\n",
    "        X_fit[filer_5_cycle]['Cycle_6'] = 1\n",
    "        X_fit = X_fit[X_fit['MarketShares_p'] != 0][self.columns]\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(X_fit.drop(['MarketShares_p'], axis=1),\n",
    "                      X_fit['MarketShares_p'])\n",
    "        self.model = reg_model\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        reg_model = self.model\n",
    "        filer_5_cycle = (X['Cycle_6']==0)&(X['Cycle_7']==0)&(X['Cycle_8']==0)&(X['Cycle_9']==0)\n",
    "        X[filer_5_cycle]['Cycle_6'] = 1\n",
    "        preds = reg_model.predict(X[X['MarketShares_p'] == 0][self.columns].drop(['MarketShares_p'], axis=1))\n",
    "        X['MarketShares_p'][X['MarketShares_p'] == 0] = preds\n",
    "        return X\n",
    "\n",
    "class RemoveColumnsUintTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        return X.drop(list(X.select_dtypes(include=['uint8']).columns), axis=1)\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        def log_transform(x):\n",
    "            return 0 if x <= 0 else np.log(x)\n",
    "        \n",
    "        X = pd.DataFrame(X)\n",
    "        for col in X:\n",
    "            X[col] = X[col].apply(log_transform)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub pipeline categorical\n",
    "cols_categorical = ['History', 'Cycle', 'Area', 'Product']\n",
    "transformer_categorical = Pipeline(steps=[\n",
    "    ('dummies', CreateDummies(drop_first = True, columns_to_left = None))\n",
    "])\n",
    "\n",
    "# sub pipeline numerical\n",
    "cols_numerical = list(X_train.columns)\n",
    "transformer_numerical = Pipeline(steps=[\n",
    "    ('create_dummies', CreateDummies(drop_first = True, columns_to_left = ['Cycle'])),\n",
    "    ('impute_missing_MarketShares_p', ImputeMissingMarketShares_p()),\n",
    "    ('remove_columns_uint_transformer', RemoveColumnsUintTransformer()),\n",
    "    ('log_transform', LogTransformer()),\n",
    "    ('first_scaler', StandardScaler()), # we have 17 columns after this\n",
    "    ('dim_red', PCA()),\n",
    "    ('second_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# transformer = numerical + categorical\n",
    "transformer = ColumnTransformer(transformers=[\n",
    "    ('categorical', transformer_categorical, cols_categorical),\n",
    "    ('numerical', transformer_numerical, cols_numerical)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(transformer.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                    ('regression', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = baseline_pipeline, name = 'baseline_pipeline')\n",
    "model_evaluation(model = baseline_pipeline, name = 'baseline_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                                 ('polynomial_features',PolynomialFeatures(degree=3)),\n",
    "                                                 ('regression_model', ElasticNet(alpha = 0.1, l1_ratio = 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 64\n",
    " \n",
    "\n",
    "max_evals = 128 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'polynomial_features__degree': hp.choice('polynomial_features__degree', np.arange(1, 4).tolist()),\n",
    "    'regression_model__alpha': hp.loguniform ('regression_model__alpha', 0.0001, 10.0),\n",
    "    'regression_model__l1_ratio': hp.uniform ('regression_model__l1_ratio', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    polynomial_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'polynomial_features__degree': space['polynomial_features__degree'],\n",
    "        'regression_model__alpha': space['regression_model__alpha'],\n",
    "        'regression_model__l1_ratio': space['regression_model__l1_ratio'],\n",
    "    }\n",
    "    \n",
    "    polynomial_regression_pipeline.set_params(**polynomial_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(polynomial_regression_pipeline, X_train, y_train, cv=10,\n",
    "                            scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "polynomial_regression = polynomial_regression_pipeline.set_params(**best_params)\n",
    "polynomial_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = polynomial_regression, name = 'polynomial_regression')\n",
    "model_evaluation(model = polynomial_regression, name = 'polynomial_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                                    ('regression_model', RandomForestRegressor(n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 128\n",
    " \n",
    "\n",
    "max_evals = 256 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'regression_model__max_depth': hp.choice('regression_model__max_depth', np.arange(2, 51).tolist()),\n",
    "    'regression_model__min_samples_leaf': hp.choice('regression_model__min_samples_leaf', \n",
    "                                                    np.arange(1, 6).tolist()),\n",
    "    'regression_model__min_samples_split': hp.choice('regression_model__min_samples_split',\n",
    "                                                     np.arange(2, 11).tolist()),\n",
    "    'regression_model__max_leaf_nodes': hp.choice('regression_model__max_leaf_nodes', np.arange(5, 31).tolist()),\n",
    "    'regression_model__n_estimators': hp.choice('regression_model__n_estimators',\n",
    "                                                np.arange(50, 1050, 50).tolist()),\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    random_forest_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'regression_model__max_depth': space['regression_model__max_depth'],\n",
    "        'regression_model__min_samples_leaf': space['regression_model__min_samples_leaf'],\n",
    "        'regression_model__min_samples_split': space['regression_model__min_samples_split'],\n",
    "        'regression_model__max_leaf_nodes': space['regression_model__max_leaf_nodes'],\n",
    "        'regression_model__n_estimators': space['regression_model__n_estimators'],\n",
    "    }\n",
    "    \n",
    "    random_forest_regression_pipeline.set_params(**random_forest_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(random_forest_regression_pipeline, X_train, y_train, cv=10,\n",
    "                              scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "random_forest_regression = random_forest_regression_pipeline.set_params(**best_params)\n",
    "random_forest_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = random_forest_regression, name = 'random_forest_regression')\n",
    "model_evaluation(model = random_forest_regression, name = 'random_forest_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Extra Trees Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                                  ('regression_model', ExtraTreesRegressor(n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 128\n",
    " \n",
    "\n",
    "max_evals = 256 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'regression_model__max_depth': hp.choice('regression_model__max_depth', np.arange(2, 51).tolist()),\n",
    "    'regression_model__min_samples_leaf': hp.choice('regression_model__min_samples_leaf', \n",
    "                                                    np.arange(1, 6).tolist()),\n",
    "    'regression_model__min_samples_split': hp.choice('regression_model__min_samples_split',\n",
    "                                                     np.arange(2, 11).tolist()),\n",
    "    'regression_model__max_leaf_nodes': hp.choice('regression_model__max_leaf_nodes', np.arange(5, 31).tolist()),\n",
    "    'regression_model__n_estimators': hp.choice('regression_model__n_estimators',\n",
    "                                                np.arange(50, 1050, 50).tolist()),\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    extra_trees_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'regression_model__max_depth': space['regression_model__max_depth'],\n",
    "        'regression_model__min_samples_leaf': space['regression_model__min_samples_leaf'],\n",
    "        'regression_model__min_samples_split': space['regression_model__min_samples_split'],\n",
    "        'regression_model__max_leaf_nodes': space['regression_model__max_leaf_nodes'],\n",
    "        'regression_model__n_estimators': space['regression_model__n_estimators'],\n",
    "    }\n",
    "    \n",
    "    extra_trees_regression_pipeline.set_params(**extra_trees_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(extra_trees_regression_pipeline, X_train, y_train, cv=10,\n",
    "                              scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "extra_trees_regression = extra_trees_regression_pipeline.set_params(**best_params)\n",
    "extra_trees_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = extra_trees_regression, name = 'extra_trees_regression')\n",
    "model_evaluation(model = extra_trees_regression, name = 'extra_trees_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                          ('regression_model', SVR(kernel='rbf'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 256\n",
    " \n",
    "max_evals = 1024 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'regression_model__C': hp.uniform ('regression_model__C', 1.0, 50000.0),\n",
    "    'regression_model__epsilon': hp.uniform ('regression_model__epsilon', 0.0001, 0.5),\n",
    "    'regression_model__gamma': hp.uniform ('regression_model__gamma', 0.0001, 100.0)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    svr_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'regression_model__C': space['regression_model__C'],\n",
    "        'regression_model__epsilon': space['regression_model__epsilon'],\n",
    "        'regression_model__gamma': space['regression_model__gamma'],\n",
    "\n",
    "    }\n",
    "    \n",
    "    svr_regression_pipeline.set_params(**svr_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(svr_regression_pipeline, X_train, y_train, cv=10,\n",
    "                              scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "svr_regression = svr_regression_pipeline.set_params(**best_params)\n",
    "svr_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = svr_regression, name = 'svr_regression')\n",
    "model_evaluation(model = svr_regression, name = 'svr_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                          ('regression_model',\n",
    "                                           AdaBoostRegressor(DecisionTreeRegressor(max_depth=3),\n",
    "                                                             n_estimators=500, learning_rate=0.5,\n",
    "                                                             random_state = 123))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 256\n",
    " \n",
    "\n",
    "max_evals = 1024 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'regression_model__n_estimators': hp.choice('regression_model__n_estimators',\n",
    "                                                np.arange(100, 1100, 100).tolist()),\n",
    "    'regression_model__learning_rate': hp.loguniform ('regression_model__learning_rate', 0.01, 0.9),\n",
    "    'regression_model__base_estimator__max_depth': hp.choice('regression_model__base_estimator__max_depth',\n",
    "                                                             np.arange(2, 21).tolist()),\n",
    "    'regression_model__base_estimator__max_leaf_nodes': hp.choice(\n",
    "                                                        'regression_model__base_estimator__max_leaf_nodes',\n",
    "                                                        np.arange(5, 31).tolist()),\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    adaboost_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'regression_model__n_estimators': space['regression_model__n_estimators'],\n",
    "        'regression_model__learning_rate': space['regression_model__learning_rate'],\n",
    "        'regression_model__base_estimator__max_depth': space['regression_model__base_estimator__max_depth'],\n",
    "        'regression_model__base_estimator__max_leaf_nodes': space[\n",
    "                                                            'regression_model__base_estimator__max_leaf_nodes'],\n",
    "    }\n",
    "    \n",
    "    adaboost_regression_pipeline.set_params(**adaboost_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(adaboost_regression_pipeline, X_train, y_train, cv=10,\n",
    "                              scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "adaboost_regression = adaboost_regression_pipeline.set_params(**best_params)\n",
    "adaboost_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = adaboost_regression, name = 'adaboost_regression')\n",
    "model_evaluation(model = adaboost_regression, name = 'adaboost_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_regression_pipeline = Pipeline(steps=[('preprocessor', transformer),\n",
    "                                          ('regression_model', xgb.XGBRegressor(n_estimators=3, learning_rate=0.5,\n",
    "                                                                                random_state=123))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startup_jobs = 512\n",
    " \n",
    "\n",
    "max_evals = 1024 \n",
    "\n",
    "space ={\n",
    "    'preprocessor__numerical__log_transform': hp.choice('preprocessor__numerical__log_transform',\n",
    "                                                        [LogTransformer(),\n",
    "                                                         PowerTransformer(method='box-cox')]),\n",
    "    'preprocessor__numerical__first_scaler': hp.choice('preprocessor__numerical__first_scaler',\n",
    "                                                       [StandardScaler(), MinMaxScaler()]),\n",
    "    'preprocessor__numerical__dim_red__n_components': hp.uniform('preprocessor__numerical__dim_red__n_components',\n",
    "                                                                0.8, 1.0),\n",
    "    'preprocessor__numerical__second_scaler': hp.choice('preprocessor__numerical__second_scaler',\n",
    "                                                       [None, StandardScaler(), MinMaxScaler()]),\n",
    "    'regression_model__n_estimators': hp.choice('regression_model__n_estimators',\n",
    "                                                np.arange(100, 1100, 100).tolist()),\n",
    "    'regression_model__learning_rate': hp.loguniform ('regression_model__learning_rate', 0.01, 0.5),\n",
    "    'regression_model__max_depth': hp.choice('regression_model__max_depth', np.arange(2, 11).tolist()),\n",
    "    'regression_model__min_child_weight': hp.choice('regression_model__min_child_weight',\n",
    "                                                    np.arange(0, 101).tolist()),\n",
    "    'regression_model__gamma': hp.loguniform('regression_model__gamma', 0.0, 2.0),\n",
    "    'regression_model__subsample': hp.uniform('regression_model__subsample', 0.5, 1.0),\n",
    "    'regression_model__colsample_bytree': hp.uniform('regression_model__colsample_bytree', 0.5, 1.0),\n",
    "    'regression_model__colsample_bylevel': hp.uniform('regression_model__colsample_bylevel', 0.5, 1.0),\n",
    "    'regression_model__reg_alpha': hp.loguniform('regression_model__reg_alpha', 0.0, 2.0),\n",
    "    'regression_model__reg_lambda': hp.loguniform('regression_model__reg_lambda', 0.0, 2.0),\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    xgboost_regression_params = {\n",
    "        'preprocessor__numerical__log_transform': space['preprocessor__numerical__log_transform'],\n",
    "        'preprocessor__numerical__first_scaler': space['preprocessor__numerical__first_scaler'],\n",
    "        'preprocessor__numerical__dim_red__n_components': space['preprocessor__numerical__dim_red__n_components'],\n",
    "        'preprocessor__numerical__second_scaler': space['preprocessor__numerical__second_scaler'],\n",
    "        'regression_model__n_estimators': space['regression_model__n_estimators'],\n",
    "        'regression_model__learning_rate': space['regression_model__learning_rate'],\n",
    "        'regression_model__max_depth': space['regression_model__max_depth'],\n",
    "        'regression_model__min_child_weight': space['regression_model__min_child_weight'],\n",
    "        'regression_model__gamma': space['regression_model__gamma'],\n",
    "        'regression_model__subsample': space['regression_model__subsample'],\n",
    "        'regression_model__colsample_bytree': space['regression_model__colsample_bytree'],\n",
    "        'regression_model__colsample_bylevel': space['regression_model__colsample_bylevel'],\n",
    "        'regression_model__reg_alpha': space['regression_model__reg_alpha'],\n",
    "        'regression_model__reg_lambda': space['regression_model__reg_lambda'],\n",
    "    }\n",
    "    \n",
    "    xgboost_regression_pipeline.set_params(**xgboost_regression_params) \n",
    "    \n",
    "    score = - cross_val_score(xgboost_regression_pipeline, X_train, y_train, cv=10,\n",
    "                              scoring = 'neg_root_mean_squared_error', n_jobs = -1).mean()\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    " \n",
    "best_params = space_eval(space, best_params)\n",
    "print('\\nThe best params:')\n",
    "print (\"{:<60} {}\".format('Parameter','Selected'))\n",
    "for k, v in best_params.items():\n",
    "    print (\"{:<60} {}\".format(k, v))\n",
    "\n",
    "xgboost_regression = xgboost_regression_pipeline.set_params(**best_params)\n",
    "xgboost_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model = xgboost_regression, name = 'xgboost_regression')\n",
    "model_evaluation(model = xgboost_regression, name = 'xgboost_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were not satisfactory, which could be due to small data and many variables. It took a lot of time to prepare the scripts to create the data set. You could use a different approach and prepare 9 models, so for each product and area separate. But the models allowed my team to reach the national final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}